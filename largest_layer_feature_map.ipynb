{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'3.9.5 (default, Aug 29 2021, 19:01:31) \\n[GCC 9.3.0]'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from sys import version\n",
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict, Counter\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "from copy import deepcopy\n",
    "from math import prod\n",
    "from sys import version\n",
    "version\n",
    "\n",
    "@dataclass\n",
    "class LayerDimensions:\n",
    "    kernel_size: Tuple[int, int]\n",
    "    stride: Tuple[int, int]\n",
    "    padding: Tuple[int, int]\n",
    "    input_size: List[int]\n",
    "    output_size: List[int]\n",
    "\n",
    "\n",
    "class ModelStatCollector:\n",
    "    def __init__(self):\n",
    "        self.model_stats = OrderedDict()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def __get_next_conv_layers(self, model):\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
    "                yield (name, module)\n",
    "\n",
    "    def __extract_stats(self, name, module, input, output):\n",
    "        self.model_stats[name] = LayerDimensions(module.kernel_size, module.stride, module.padding, input_size=list(\n",
    "            input[0].size()), output_size=list(output[0].size()))\n",
    "\n",
    "    def __attach_collection_hooks_to_model(self, model):\n",
    "\n",
    "        for name, conv_layer in self.__get_next_conv_layers(model):\n",
    "            layer_collector = partial(self.__extract_stats, name)\n",
    "            self.hooks.append(\n",
    "                conv_layer.register_forward_hook(layer_collector))\n",
    "\n",
    "    def __detach_stats_collection_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def __reset(self):\n",
    "        self.model_stats = {}\n",
    "        self.hooks = []\n",
    "\n",
    "    def collect_stats_from_model(self, model, input_batch):\n",
    "        self.__attach_collection_hooks_to_model(model)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            model(input_batch)\n",
    "        self.__detach_stats_collection_hooks()\n",
    "        collected_stats = deepcopy(self.model_stats)\n",
    "        self.__reset()\n",
    "        return collected_stats\n",
    "\n",
    "class ModelStatAnalyser:\n",
    "    @ classmethod\n",
    "    def get_kernel_stats(cls, model_stats):\n",
    "        kernel_size_counter = Counter()\n",
    "        stride_counter_dict = {}\n",
    "        for layer in model_stats.values():\n",
    "            kernel_size = layer.kernel_size\n",
    "            stride = layer.stride\n",
    "            kernel_size_counter.update(str(kernel_size[0]))\n",
    "            if str(kernel_size[0]) not in stride_counter_dict:\n",
    "                stride_counter_dict[str(kernel_size[0])] = Counter()\n",
    "            stride_counter_dict[str(kernel_size[0])].update(str(stride[0]))\n",
    "        return kernel_size_counter\n",
    "\n",
    "    @ classmethod\n",
    "    def get_intermediate_layer_sizes(cls, model_stats):\n",
    "        intermediate_layer_sizes = [\n",
    "            prod(layer.input_size) for layer in model_stats.values()]\n",
    "        intermediate_layer_sizes.append(\n",
    "            prod(list(model_stats.values())[-1].output_size))\n",
    "        return intermediate_layer_sizes\n",
    "\n",
    "    @ classmethod\n",
    "    def get_intermediate_layer_size_bounds(cls, model_stats):\n",
    "        intermediate_layer_sizes = cls.get_intermediate_layer_sizes(model_stats)\n",
    "        return (max(intermediate_layer_sizes),\n",
    "                min(intermediate_layer_sizes))\n",
    "\n",
    "    @ classmethod\n",
    "    def get_ub_input_size(cls, model_stats):\n",
    "        return max([prod(layer.kernel_size[0]) for layer in model_stats.values()])\n",
    "\n",
    "    @ classmethod\n",
    "    def get_in_channel_stats(cls, model_stats):\n",
    "        in_channel_dict = {}\n",
    "        for layer in model_stats.values():\n",
    "            kernel_size = layer.kernel_size\n",
    "            if str(kernel_size[0]) not in in_channel_dict:\n",
    "                in_channel_dict[str(kernel_size[0])] = {}\n",
    "            if str(layer.input_size[1]) not in in_channel_dict[str(kernel_size[0])]:\n",
    "                in_channel_dict[str(kernel_size[0])][str(layer.input_size[1])] = 0\n",
    "            in_channel_dict[str(kernel_size[0])][str(layer.input_size[1])] += 1\n",
    "\n",
    "        return in_channel_dict\n",
    "\n",
    "    @ classmethod\n",
    "    def get_filter_stats(cls, model_stats):\n",
    "        out_channel_dict = {}\n",
    "        for layer in model_stats.values():\n",
    "            kernel_size = layer.kernel_size\n",
    "            if str(kernel_size[0]) not in out_channel_dict:\n",
    "                out_channel_dict[str(kernel_size[0])] = {}\n",
    "            if str(layer.output_size[0]) not in out_channel_dict[str(kernel_size[0])]:\n",
    "                out_channel_dict[str(kernel_size[0])][str(layer.output_size[0])] = 0\n",
    "            out_channel_dict[str(kernel_size[0])][str(layer.output_size[0])] += 1\n",
    "\n",
    "        return out_channel_dict\n",
    "\n",
    "    @ classmethod\n",
    "    def get_stride_stats(cls, model_stats):\n",
    "        stride_counter_dict = {}\n",
    "        for layer in model_stats.values():\n",
    "            kernel_size = layer.kernel_size\n",
    "            stride = layer.stride\n",
    "            if str(kernel_size[0]) not in stride_counter_dict:\n",
    "                stride_counter_dict[str(kernel_size[0])] = Counter()\n",
    "            stride_counter_dict[str(kernel_size[0])].update(str(stride[0]))\n",
    "        return stride_counter_dict\n",
    "\n",
    "    @ classmethod \n",
    "    def get_models_stats_dict(cls, model_dict, input_batch, ssd_input_batch = None):\n",
    "        collector = ModelStatCollector()\n",
    "        stats_dict = {}\n",
    "        raw_stats_dict = {}\n",
    "        if torch.cuda.is_available():\n",
    "            input_batch = input_batch.to('cuda')\n",
    "            if ssd_input_batch is not None:\n",
    "                ssd_input_batch = ssd_input_batch.to('cuda')\n",
    "        for model_name, model in model_dict.items():\n",
    "            print('Analysing {}'.format(model_name))\n",
    "            model.to('cuda')\n",
    "            if model_name == 'ssd' and ssd_input_batch is not None:\n",
    "                model_stats = collector.collect_stats_from_model(model, ssd_input_batch)\n",
    "            else:\n",
    "                model_stats = collector.collect_stats_from_model(model, input_batch)\n",
    "                \n",
    "            model.to('cpu')\n",
    "            raw_stats_dict[model_name] = model_stats\n",
    "            stats_dict[model_name] = {'kernel': ModelStatAnalyser.get_kernel_stats(model_stats),\n",
    "                                    'stride': ModelStatAnalyser.get_stride_stats(model_stats),\n",
    "                                    'in_channel': ModelStatAnalyser.get_in_channel_stats(model_stats),\n",
    "                                    'filters': ModelStatAnalyser.get_filter_stats(model_stats),\n",
    "                                    'intermediate_layer_bounds': ModelStatAnalyser.get_intermediate_layer_size_bounds(model_stats)}\n",
    "        return stats_dict, raw_stats_dict\n",
    "    \n",
    "class ModelStatsAggregator:\n",
    "    \n",
    "    @ classmethod\n",
    "    def get_aggregate_kernel_stats_as_percentages(cls, stats_dict):\n",
    "        aggregate_kernel_stats = Counter()\n",
    "        for model, stats in stats_dict.items():\n",
    "            aggregate_kernel_stats += stats['kernel']\n",
    "        ksize, counts = zip(*[(ksize, count) for ksize, count in aggregate_kernel_stats.items()])\n",
    "        total_kernels = sum(counts)\n",
    "        aggregate_kernel_stats_percentages = {ksize: counts/total_kernels for ksize, counts in zip(ksize,counts)}\n",
    "        return aggregate_kernel_stats_percentages\n",
    "\n",
    "    @ classmethod\n",
    "    def get_aggregate_stride_stats_per_kernel(cls, stats_dict):\n",
    "        aggregate_stride_stats = {}\n",
    "        for model, stats in stats_dict.items():\n",
    "            for kernel, counter in stats['stride'].items():\n",
    "                if kernel not in aggregate_stride_stats:\n",
    "                    aggregate_stride_stats[kernel] = Counter()\n",
    "                aggregate_stride_stats[kernel] += counter\n",
    "        return aggregate_stride_stats\n",
    "    \n",
    "    @ classmethod\n",
    "    def get_stride_stats_per_kernel_as_percentages(cls, stats_dict):\n",
    "        aggregate_stride_stats = cls.get_aggregate_stride_stats_per_kernel(stats_dict)\n",
    "        aggregate_stride_stats_percentages = {}\n",
    "        for ksize, stride_counter in aggregate_stride_stats.items():\n",
    "            total_kernels = sum(stride_counter.values())\n",
    "            aggregate_stride_stats_percentages[ksize] = {stride: count/total_kernels for stride, count in dict(stride_counter).items()}\n",
    "        return aggregate_stride_stats_percentages\n",
    "    \n",
    "    @ classmethod\n",
    "    def get_aggregate_in_channel_stats_per_kernel(cls, stats_dict):\n",
    "        aggregate_in_channel_stats = {}\n",
    "        for _, stats in stats_dict.items():\n",
    "            for kernel, channel_dict in stats['in_channel'].items():\n",
    "                if kernel not in aggregate_in_channel_stats:\n",
    "                    aggregate_in_channel_stats[kernel] = {}\n",
    "                for channel, count in channel_dict.items():\n",
    "                    if channel not in aggregate_in_channel_stats[kernel]:\n",
    "                        aggregate_in_channel_stats[kernel][channel] = 0\n",
    "                    aggregate_in_channel_stats[kernel][channel] += count\n",
    "        for kernel, channel_dict in aggregate_in_channel_stats.items():\n",
    "            channel_dict = {k: v for k, v in sorted(channel_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "            aggregate_in_channel_stats[kernel] = channel_dict\n",
    "        return aggregate_in_channel_stats\n",
    "    \n",
    "    @ classmethod\n",
    "    def get_aggregate_in_channel_stats(cls, stats_dict):\n",
    "        aggregate_in_channel_stats = cls.get_aggregate_in_channel_stats_per_kernel(stats_dict)\n",
    "        cross_kernel_aggregate_channel = {}\n",
    "        for channel_dicts in aggregate_in_channel_stats.values():\n",
    "            for channel_size, count in channel_dicts.items():\n",
    "                if channel_size not in cross_kernel_aggregate_channel:\n",
    "                    cross_kernel_aggregate_channel[channel_size] = 0\n",
    "                cross_kernel_aggregate_channel[channel_size] += count\n",
    "        cross_kernel_aggregate_channel = {k: v for k,v in sorted(cross_kernel_aggregate_channel.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return cross_kernel_aggregate_channel\n",
    "        \n",
    "    @ classmethod\n",
    "    def get_aggregate_filter_stats_per_kernel(cls, stats_dict):\n",
    "        aggregate_filter_stats = {}\n",
    "        for model, stats in stats_dict.items():\n",
    "            for kernel, channel_dict in stats['filters'].items():\n",
    "                if kernel not in aggregate_filter_stats:\n",
    "                    aggregate_filter_stats[kernel] = {}\n",
    "                for channel, count in channel_dict.items():\n",
    "                    if channel not in aggregate_filter_stats[kernel]:\n",
    "                        aggregate_filter_stats[kernel][channel] = 0\n",
    "                    aggregate_filter_stats[kernel][channel] += count\n",
    "        for kernel, channel_dict in aggregate_filter_stats.items():\n",
    "            channel_dict = {k: v for k, v in sorted(channel_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "            aggregate_filter_stats[kernel] = channel_dict\n",
    "        return aggregate_filter_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sultan/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "Using cache found in /home/sultan/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7266973 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 308 layers, 21356877 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 392 layers, 47025981 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 87730285 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model_dict = {\n",
    "    'ssd': torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd'),\n",
    "    'lenet': torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True),\n",
    "    'yolov5s': torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True),\n",
    "    'yolov5m': torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True),\n",
    "    'yolov5l': torch.hub.load('ultralytics/yolov5', 'yolov5l', pretrained=True),\n",
    "    'yolov5x': torch.hub.load('ultralytics/yolov5', 'yolov5x', pretrained=True),\n",
    "    'alexnet': models.alexnet(pretrained=True, progress=True),\n",
    "    'vgg_11': models.vgg11(pretrained=True, progress=True),\n",
    "    'vgg_13': models.vgg13(pretrained=True, progress=True),\n",
    "    'vgg_16': models.vgg16(pretrained=True, progress=True),\n",
    "    'vgg_19': models.vgg19(pretrained=True, progress=True),\n",
    "    'vgg_11_bn': models.vgg11_bn(pretrained=True, progress=True),\n",
    "    'vgg_13_bn': models.vgg13_bn(pretrained=True, progress=True),\n",
    "    'vgg_16_bn': models.vgg16_bn(pretrained=True, progress=True),\n",
    "    'vgg_19_bn': models.vgg19_bn(pretrained=True, progress=True),\n",
    "    'resnet_18': models.resnet18(pretrained=True, progress=True),\n",
    "    'resnet_34': models.resnet34(pretrained=True, progress=True),\n",
    "    'resnet_50': models.resnet50(pretrained=True, progress=True),\n",
    "    'resnet_101': models.resnet101(pretrained=True, progress=True),\n",
    "    'resnet_152': models.resnet152(pretrained=True, progress=True),\n",
    "    'squeezenet_1_0': models.squeezenet1_1(pretrained=True, progress=True),\n",
    "    'squeezenet_1_1': models.squeezenet1_0(pretrained=True, progress=True),\n",
    "    'densenet_121': models.densenet121(pretrained=True, progress=True),\n",
    "    'densenet_169': models.densenet169(pretrained=True, progress=True),\n",
    "    'densenet_201': models.densenet201(pretrained=True, progress=True),\n",
    "    'densenet_161': models.densenet161(pretrained=True, progress=True),\n",
    "    'inception_v3': models.inception_v3(pretrained=True, progress=True),\n",
    "    'googlenet': models.googlenet(pretrained=True, progress=True),\n",
    "    'shufflenet_v2_x0_5': models.shufflenet_v2_x0_5(pretrained=True, progress=True),\n",
    "    'shufflenet_v2_x1_0': models.shufflenet_v2_x1_0(pretrained=True, progress=True),\n",
    "    'mobilenet_v2': models.mobilenet_v2(pretrained=True, progress=True),\n",
    "    'mobilenet_v3_large': models.mobilenet_v3_large(pretrained=True, progress=True),\n",
    "    'mobilenet_v3_small': models.mobilenet_v3_small(pretrained=True, progress=True),\n",
    "    'resnext_50_32x4d': models.resnext50_32x4d(pretrained=True, progress=True),\n",
    "    'resnext_101_32x8d': models.resnext101_32x8d(pretrained=True, progress=True),\n",
    "    'wide_resnet_50_2': models.wide_resnet50_2(pretrained=True, progress=True),\n",
    "    'wide_resnet_101_2': models.wide_resnet101_2(pretrained=True, progress=True),\n",
    "    'mnasnet0_5': models.mnasnet0_5(pretrained=True, progress=True),\n",
    "    'mnasnet1_0': models.mnasnet1_0(pretrained=True, progress=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sultan/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "# prepare sample inputs\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "uris = [\n",
    "    'http://images.cocodataset.org/val2017/000000397133.jpg'\n",
    "]\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n",
    "\n",
    "inputs = [utils.prepare_input(uri) for uri in uris]\n",
    "ssd_input_batch = utils.prepare_tensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing ssd\n",
      "Analysing lenet\n",
      "Analysing yolov5s\n",
      "Analysing yolov5m\n",
      "Analysing yolov5l\n",
      "Analysing yolov5x\n",
      "Analysing alexnet\n",
      "Analysing vgg_11\n",
      "Analysing vgg_13\n",
      "Analysing vgg_16\n",
      "Analysing vgg_19\n",
      "Analysing vgg_11_bn\n",
      "Analysing vgg_13_bn\n",
      "Analysing vgg_16_bn\n",
      "Analysing vgg_19_bn\n",
      "Analysing resnet_18\n",
      "Analysing resnet_34\n",
      "Analysing resnet_50\n",
      "Analysing resnet_101\n",
      "Analysing resnet_152\n",
      "Analysing squeezenet_1_0\n",
      "Analysing squeezenet_1_1\n",
      "Analysing densenet_121\n",
      "Analysing densenet_169\n",
      "Analysing densenet_201\n",
      "Analysing densenet_161\n",
      "Analysing inception_v3\n",
      "Analysing googlenet\n",
      "Analysing shufflenet_v2_x0_5\n",
      "Analysing shufflenet_v2_x1_0\n",
      "Analysing mobilenet_v2\n",
      "Analysing mobilenet_v3_large\n",
      "Analysing mobilenet_v3_small\n",
      "Analysing resnext_50_32x4d\n",
      "Analysing resnext_101_32x8d\n",
      "Analysing wide_resnet_50_2\n",
      "Analysing wide_resnet_101_2\n",
      "Analysing mnasnet0_5\n",
      "Analysing mnasnet1_0\n"
     ]
    }
   ],
   "source": [
    "stats_dict, _ = ModelStatAnalyser.get_models_stats_dict(model_dict, input_batch, ssd_input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'7': 0.011862396204033215,\n '1': 0.5776986951364176,\n '3': 0.39541320680110714,\n '5': 0.015025701858442072}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelStatsAggregator.get_aggregate_kernel_stats_as_percentages(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'7': Counter({'2': 17, '1': 13}),\n '1': Counter({'1': 1431, '2': 28}),\n '3': Counter({'1': 910, '2': 90}),\n '11': Counter({'4': 1}),\n '5': Counter({'1': 28, '2': 10})}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelStatsAggregator.get_aggregate_stride_stats_per_kernel(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'128': 392,\n '256': 334,\n '512': 261,\n '1024': 219,\n '192': 149,\n '64': 117,\n '160': 65,\n '96': 64,\n '48': 56,\n '320': 55,\n '384': 51,\n '24': 37,\n '768': 36,\n '3': 35,\n '640': 34,\n '32': 31,\n '576': 30,\n '116': 26,\n '16': 26,\n '2048': 25,\n '480': 25,\n '832': 21,\n '288': 21,\n '240': 20,\n '1280': 19,\n '960': 19,\n '72': 18,\n '80': 17,\n '1152': 14,\n '232': 14,\n '120': 14,\n '144': 14,\n '40': 13,\n '672': 13,\n '58': 11,\n '528': 10,\n '448': 8,\n '1536': 6,\n '224': 6,\n '352': 6,\n '416': 6,\n '864': 6,\n '896': 6,\n '928': 6,\n '992': 6,\n '1056': 6,\n '1248': 6,\n '704': 5,\n '736': 5,\n '800': 5,\n '1344': 5,\n '1440': 5,\n '1632': 5,\n '544': 4,\n '608': 4,\n '1088': 4,\n '1120': 4,\n '1184': 4,\n '1216': 4,\n '1728': 4,\n '184': 4,\n '112': 4,\n '12': 4,\n '1312': 3,\n '1376': 3,\n '1408': 3,\n '1472': 3,\n '1504': 3,\n '1568': 3,\n '1600': 3,\n '1824': 3,\n '1664': 2,\n '1696': 2,\n '1760': 2,\n '1792': 2,\n '336': 2,\n '432': 2,\n '624': 2,\n '720': 2,\n '1104': 2,\n '1200': 2,\n '1296': 2,\n '1392': 2,\n '1488': 2,\n '1584': 2,\n '1680': 2,\n '1776': 2,\n '1872': 2,\n '1920': 2,\n '1968': 2,\n '2016': 2,\n '2064': 2,\n '2112': 2,\n '168': 2,\n '8': 2,\n '200': 2,\n '88': 2,\n '2560': 1,\n '1856': 1,\n '1888': 1,\n '816': 1,\n '912': 1,\n '1008': 1,\n '2160': 1,\n '464': 1}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelStatsAggregator.get_aggregate_in_channel_stats(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('.venv': poetry)",
   "name": "python2717jvsc74a57bd07ba5a8c11295bd31a650da7fd95bdbf05f90d3e784de664cc7b5246ae59e510d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "7ba5a8c11295bd31a650da7fd95bdbf05f90d3e784de664cc7b5246ae59e510d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}