{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'3.9.5 (default, Aug 29 2021, 19:01:31) \\n[GCC 9.3.0]'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from collections import OrderedDict, Counter\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "from copy import deepcopy\n",
    "from math import prod\n",
    "from sys import version\n",
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sultan/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "Using cache found in /home/sultan/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7266973 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 308 layers, 21356877 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 392 layers, 47025981 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 87730285 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model_dict = {\n",
    "    'ssd': torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd'),\n",
    "    'lenet': torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True),\n",
    "    'yolov5s': torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True),\n",
    "    'yolov5m': torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True),\n",
    "    'yolov5l': torch.hub.load('ultralytics/yolov5', 'yolov5l', pretrained=True),\n",
    "    'yolov5x': torch.hub.load('ultralytics/yolov5', 'yolov5x', pretrained=True),\n",
    "    'alexnet': models.alexnet(pretrained=True, progress=True),\n",
    "    'vgg_11': models.vgg11(pretrained=True, progress=True),\n",
    "    'vgg_13': models.vgg13(pretrained=True, progress=True),\n",
    "    'vgg_16': models.vgg16(pretrained=True, progress=True),\n",
    "    'vgg_19': models.vgg19(pretrained=True, progress=True),\n",
    "    'vgg_11_bn': models.vgg11_bn(pretrained=True, progress=True),\n",
    "    'vgg_13_bn': models.vgg13_bn(pretrained=True, progress=True),\n",
    "    'vgg_16_bn': models.vgg16_bn(pretrained=True, progress=True),\n",
    "    'vgg_19_bn': models.vgg19_bn(pretrained=True, progress=True),\n",
    "    'resnet_18': models.resnet18(pretrained=True, progress=True),\n",
    "    'resnet_34': models.resnet34(pretrained=True, progress=True),\n",
    "    'resnet_50': models.resnet50(pretrained=True, progress=True),\n",
    "    'resnet_101': models.resnet101(pretrained=True, progress=True),\n",
    "    'resnet_152': models.resnet152(pretrained=True, progress=True),\n",
    "    'squeezenet_1_0': models.squeezenet1_1(pretrained=True, progress=True),\n",
    "    'squeezenet_1_1': models.squeezenet1_0(pretrained=True, progress=True),\n",
    "    'densenet_121': models.densenet121(pretrained=True, progress=True),\n",
    "    'densenet_169': models.densenet169(pretrained=True, progress=True),\n",
    "    'densenet_201': models.densenet201(pretrained=True, progress=True),\n",
    "    'densenet_161': models.densenet161(pretrained=True, progress=True),\n",
    "    'inception_v3': models.inception_v3(pretrained=True, progress=True),\n",
    "    'googlenet': models.googlenet(pretrained=True, progress=True),\n",
    "    'shufflenet_v2_x0_5': models.shufflenet_v2_x0_5(pretrained=True, progress=True),\n",
    "    'shufflenet_v2_x1_0': models.shufflenet_v2_x1_0(pretrained=True, progress=True),\n",
    "    'mobilenet_v2': models.mobilenet_v2(pretrained=True, progress=True),\n",
    "    'mobilenet_v3_large': models.mobilenet_v3_large(pretrained=True, progress=True),\n",
    "    'mobilenet_v3_small': models.mobilenet_v3_small(pretrained=True, progress=True),\n",
    "    'resnext_50_32x4d': models.resnext50_32x4d(pretrained=True, progress=True),\n",
    "    'resnext_101_32x8d': models.resnext101_32x8d(pretrained=True, progress=True),\n",
    "    'wide_resnet_50_2': models.wide_resnet50_2(pretrained=True, progress=True),\n",
    "    'wide_resnet_101_2': models.wide_resnet101_2(pretrained=True, progress=True),\n",
    "    'mnasnet0_5': models.mnasnet0_5(pretrained=True, progress=True),\n",
    "    'mnasnet1_0': models.mnasnet1_0(pretrained=True, progress=True),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sultan/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image from the pytorch website\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "uris = [\n",
    "    'http://images.cocodataset.org/val2017/000000397133.jpg'\n",
    "]\n",
    "inputs = [utils.prepare_input(uri) for uri in uris]\n",
    "ssd_input_batch = utils.prepare_tensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_conv_layers(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
    "            yield (name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LayerDimensions:\n",
    "    kernel_size: Tuple[int, int]\n",
    "    stride: Tuple[int, int]\n",
    "    padding: Tuple[int, int]\n",
    "    input_size: List[int]\n",
    "    output_size: List[int]\n",
    "\n",
    "\n",
    "class ModelStatCollector:\n",
    "    def __init__(self):\n",
    "        self.model_stats = OrderedDict()\n",
    "        self.hooks = []\n",
    "\n",
    "    def __extract_stats(self, name, module, input, output):\n",
    "        self.model_stats[name] = LayerDimensions(module.kernel_size, module.stride, module.padding, input_size=list(\n",
    "            input[0].size()), output_size=list(output[0].size()))\n",
    "\n",
    "    def __attach_collection_hooks_to_model(self, model):\n",
    "\n",
    "        for name, conv_layer in get_next_conv_layers(model):\n",
    "            layer_collector = partial(self.__extract_stats, name)\n",
    "            self.hooks.append(\n",
    "                conv_layer.register_forward_hook(layer_collector))\n",
    "\n",
    "    def __detach_stats_collection_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def __reset(self):\n",
    "        self.model_stats = {}\n",
    "        self.hooks = []\n",
    "\n",
    "    def collect_stats_from_model(self, model, input_batch):\n",
    "        self.__attach_collection_hooks_to_model(model)\n",
    "        model.eval()\n",
    "        # move the input and model to GPU for speed if available\n",
    "        with torch.no_grad():\n",
    "            model(input_batch)\n",
    "        self.__detach_stats_collection_hooks()\n",
    "        collected_stats = deepcopy(self.model_stats)\n",
    "        self.__reset()\n",
    "        return collected_stats\n",
    "\n",
    "\n",
    "class ModelStatAnalyser:\n",
    "    @ classmethod\n",
    "    def get_kernel_stats(cls, model_stats):\n",
    "        kernel_size_counter = Counter()\n",
    "        stride_counter_dict = {}\n",
    "        for layer in model_stats.values():\n",
    "            kernel_size = layer.kernel_size\n",
    "            stride = layer.stride\n",
    "            kernel_size_counter.update(str(kernel_size[0]))\n",
    "            if str(kernel_size[0]) not in stride_counter_dict:\n",
    "                stride_counter_dict[str(kernel_size[0])] = Counter()\n",
    "            stride_counter_dict[str(kernel_size[0])].update(str(stride[0]))\n",
    "        return kernel_size_counter\n",
    "\n",
    "    @ classmethod\n",
    "    def get_intermediate_layer_sizes(cls, model_stats):\n",
    "        intermediate_layer_sizes = [\n",
    "            prod(layer.input_size) for layer in model_stats.values()]\n",
    "        intermediate_layer_sizes.append(\n",
    "            prod(list(model_stats.values())[-1].output_size))\n",
    "        return intermediate_layer_sizes\n",
    "\n",
    "    @ classmethod\n",
    "    def get_intermediate_layer_size_bounds(cls, model_stats):\n",
    "        return (max(cls.get_intermediate_layer_sizes(model_stats)),\n",
    "                min(cls.get_intermediate_layer_sizes(model_stats)))\n",
    "\n",
    "    @ classmethod\n",
    "    def get_ub_input_size(cls, model_stats):\n",
    "        return max([prod(layer.kernel_size[0]) for layer in model_stats.values()])\n",
    "\n",
    "    @ classmethod\n",
    "    def get_stride_stats(cls, model_stats):\n",
    "        stride_counter_dict = {}\n",
    "\n",
    "        for layer in model_stats.values():\n",
    "            kernel_size = layer.kernel_size\n",
    "            stride = layer.stride\n",
    "            if str(kernel_size[0]) not in stride_counter_dict:\n",
    "                stride_counter_dict[str(kernel_size[0])] = Counter()\n",
    "            stride_counter_dict[str(kernel_size[0])].update(str(stride[0]))\n",
    "        return stride_counter_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing ssd\n",
      "Analysing lenet\n",
      "Analysing yolov5s\n",
      "Analysing yolov5m\n",
      "Analysing yolov5l\n",
      "Analysing yolov5x\n",
      "Analysing alexnet\n",
      "Analysing vgg_11\n",
      "Analysing vgg_13\n",
      "Analysing vgg_16\n",
      "Analysing vgg_19\n",
      "Analysing vgg_11_bn\n",
      "Analysing vgg_13_bn\n",
      "Analysing vgg_16_bn\n",
      "Analysing vgg_19_bn\n",
      "Analysing resnet_18\n",
      "Analysing resnet_34\n",
      "Analysing resnet_50\n",
      "Analysing resnet_101\n",
      "Analysing resnet_152\n",
      "Analysing squeezenet_1_0\n",
      "Analysing squeezenet_1_1\n",
      "Analysing densenet_121\n",
      "Analysing densenet_169\n",
      "Analysing densenet_201\n",
      "Analysing densenet_161\n",
      "Analysing inception_v3\n",
      "Analysing googlenet\n",
      "Analysing shufflenet_v2_x0_5\n",
      "Analysing shufflenet_v2_x1_0\n",
      "Analysing mobilenet_v2\n",
      "Analysing mobilenet_v3_large\n",
      "Analysing mobilenet_v3_small\n",
      "Analysing resnext_50_32x4d\n",
      "Analysing resnext_101_32x8d\n",
      "Analysing wide_resnet_50_2\n",
      "Analysing wide_resnet_101_2\n",
      "Analysing mnasnet0_5\n",
      "Analysing mnasnet1_0\n"
     ]
    }
   ],
   "source": [
    "collector = ModelStatCollector()\n",
    "stats_dict = {}\n",
    "raw_stats_dict = {}\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    ssd_input_batch = ssd_input_batch.to('cuda')\n",
    "for model_name, model in model_dict.items():\n",
    "    print('Analysing {}'.format(model_name))\n",
    "    model.to('cuda')\n",
    "    if model_name == 'ssd':\n",
    "        model_stats = collector.collect_stats_from_model(model, ssd_input_batch)\n",
    "    else:\n",
    "        model_stats = collector.collect_stats_from_model(model, input_batch)\n",
    "        \n",
    "    model.to('cpu')\n",
    "    raw_stats_dict[model_name] = model_stats\n",
    "    stats_dict[model_name] = {'kernel': ModelStatAnalyser.get_kernel_stats(model_stats),\n",
    "                              'stride': ModelStatAnalyser.get_stride_stats(model_stats),\n",
    "                              'intermediate_layer_bounds': ModelStatAnalyser.get_intermediate_layer_size_bounds(model_stats)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'7': 0.011862396204033215,\n '1': 0.5776986951364176,\n '3': 0.39541320680110714,\n '5': 0.015025701858442072}"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_kernel_stats = Counter()\n",
    "for model, stats in stats_dict.items():\n",
    "    aggregate_kernel_stats += stats['kernel']\n",
    "ksize, counts = zip(*[(ksize, count) for ksize, count in aggregate_kernel_stats.items()])\n",
    "total_kernels = sum(counts)\n",
    "aggregate_kernel_stats_percentages = {ksize: counts/total_kernels for ksize, counts in zip(ksize,counts)}\n",
    "aggregate_kernel_stats_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'7': {'2': 0.5666666666666667, '1': 0.43333333333333335},\n '1': {'1': 0.9808087731322824, '2': 0.019191226867717615},\n '3': {'1': 0.91, '2': 0.09},\n '11': {'4': 1.0},\n '5': {'1': 0.7368421052631579, '2': 0.2631578947368421}}"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_stride_stats = {}\n",
    "for model, stats in stats_dict.items():\n",
    "    for kernel, counter in stats['stride'].items():\n",
    "        if kernel not in aggregate_stride_stats:\n",
    "            aggregate_stride_stats[kernel] = Counter()\n",
    "        aggregate_stride_stats[kernel] += counter\n",
    "\n",
    "aggregate_stride_stats_percentages = {}\n",
    "for ksize, stride_counter in aggregate_stride_stats.items():\n",
    "    total_kernels = sum(stride_counter.values())\n",
    "    aggregate_stride_stats_percentages[ksize] = {stride: count/total_kernels for stride, count in dict(stride_counter).items()}\n",
    "aggregate_stride_stats_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layers in raw_stats_dict.items():\n",
    "    for layer_name, layer in layers.items():\n",
    "        if layer.kernel_size[0] == 1 and layer.stride[0] == 2:\n",
    "            print(\"model {}, has kernel of size 1 with stide 2: {}\".format(name, layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'model.model.0.conv.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 12, 112, 112], output_size=[64, 112, 112]),\n 'model.model.1.conv': LayerDimensions(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), input_size=[1, 64, 112, 112], output_size=[128, 56, 56]),\n 'model.model.2.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 56, 56], output_size=[64, 56, 56]),\n 'model.model.2.m.0.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 64, 56, 56], output_size=[64, 56, 56]),\n 'model.model.2.m.0.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 64, 56, 56], output_size=[64, 56, 56]),\n 'model.model.2.m.1.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 64, 56, 56], output_size=[64, 56, 56]),\n 'model.model.2.m.1.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 64, 56, 56], output_size=[64, 56, 56]),\n 'model.model.2.m.2.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 64, 56, 56], output_size=[64, 56, 56]),\n 'model.model.2.m.2.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 64, 56, 56], output_size=[64, 56, 56]),\n 'model.model.2.cv2.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 56, 56], output_size=[64, 56, 56]),\n 'model.model.2.cv3.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 56, 56], output_size=[128, 56, 56]),\n 'model.model.3.conv': LayerDimensions(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), input_size=[1, 128, 56, 56], output_size=[256, 28, 28]),\n 'model.model.4.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.0.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.0.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.1.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.1.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.2.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.2.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.3.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.3.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.4.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.4.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.5.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.5.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.6.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.6.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.7.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.7.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.8.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.m.8.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.cv2.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 28, 28], output_size=[128, 28, 28]),\n 'model.model.4.cv3.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 28, 28], output_size=[256, 28, 28]),\n 'model.model.5.conv': LayerDimensions(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), input_size=[1, 256, 28, 28], output_size=[512, 14, 14]),\n 'model.model.6.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.0.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.0.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.1.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.1.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.2.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.2.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.3.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.3.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.4.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.4.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.5.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.5.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.6.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.6.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.7.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.7.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.8.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.m.8.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.cv2.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 14, 14], output_size=[256, 14, 14]),\n 'model.model.6.cv3.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 14, 14], output_size=[512, 14, 14]),\n 'model.model.7.conv': LayerDimensions(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), input_size=[1, 512, 14, 14], output_size=[1024, 7, 7]),\n 'model.model.8.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 1024, 7, 7], output_size=[512, 7, 7]),\n 'model.model.8.cv2.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 2048, 7, 7], output_size=[1024, 7, 7]),\n 'model.model.9.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 1024, 7, 7], output_size=[512, 7, 7]),\n 'model.model.9.m.0.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.9.m.0.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.9.m.1.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.9.m.1.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.9.m.2.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.9.m.2.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.9.cv2.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 1024, 7, 7], output_size=[512, 7, 7]),\n 'model.model.9.cv3.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 1024, 7, 7], output_size=[1024, 7, 7]),\n 'model.model.10.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 1024, 7, 7], output_size=[512, 7, 7]),\n 'model.model.13.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 1024, 14, 14], output_size=[256, 14, 14]),\n 'model.model.13.m.0.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.13.m.0.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.13.m.1.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.13.m.1.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.13.m.2.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.13.m.2.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.13.cv2.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 1024, 14, 14], output_size=[256, 14, 14]),\n 'model.model.13.cv3.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 14, 14], output_size=[512, 14, 14]),\n 'model.model.14.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 14, 14], output_size=[256, 14, 14]),\n 'model.model.17.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 28, 28], output_size=[128, 28, 28]),\n 'model.model.17.m.0.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.17.m.0.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.17.m.1.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.17.m.1.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.17.m.2.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.17.m.2.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 128, 28, 28], output_size=[128, 28, 28]),\n 'model.model.17.cv2.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 28, 28], output_size=[128, 28, 28]),\n 'model.model.17.cv3.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 28, 28], output_size=[256, 28, 28]),\n 'model.model.18.conv': LayerDimensions(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), input_size=[1, 256, 28, 28], output_size=[256, 14, 14]),\n 'model.model.20.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 14, 14], output_size=[256, 14, 14]),\n 'model.model.20.m.0.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.20.m.0.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.20.m.1.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.20.m.1.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.20.m.2.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.20.m.2.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 256, 14, 14], output_size=[256, 14, 14]),\n 'model.model.20.cv2.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 14, 14], output_size=[256, 14, 14]),\n 'model.model.20.cv3.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 14, 14], output_size=[512, 14, 14]),\n 'model.model.21.conv': LayerDimensions(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), input_size=[1, 512, 14, 14], output_size=[512, 7, 7]),\n 'model.model.23.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 1024, 7, 7], output_size=[512, 7, 7]),\n 'model.model.23.m.0.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.23.m.0.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.23.m.1.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.23.m.1.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.23.m.2.cv1.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.23.m.2.cv2.conv': LayerDimensions(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), input_size=[1, 512, 7, 7], output_size=[512, 7, 7]),\n 'model.model.23.cv2.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 1024, 7, 7], output_size=[512, 7, 7]),\n 'model.model.23.cv3.conv': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 1024, 7, 7], output_size=[1024, 7, 7]),\n 'model.model.24.m.0': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 256, 28, 28], output_size=[255, 28, 28]),\n 'model.model.24.m.1': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 512, 14, 14], output_size=[255, 14, 14]),\n 'model.model.24.m.2': LayerDimensions(kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), input_size=[1, 1024, 7, 7], output_size=[255, 7, 7])}"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_stats_dict['yolov5l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('.venv': poetry)",
   "name": "pythonjvsc74a57bd07ba5a8c11295bd31a650da7fd95bdbf05f90d3e784de664cc7b5246ae59e510d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "7ba5a8c11295bd31a650da7fd95bdbf05f90d3e784de664cc7b5246ae59e510d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}