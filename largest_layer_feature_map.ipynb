{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from ModelAnalysis import ModelStatsAggregator, ModelStatAnalyser\n",
    "from sys import version\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import urllib\n",
    "from PIL import Image\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "version"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'3.9.5 (default, Aug 29 2021, 19:01:31) \\n[GCC 9.3.0]'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model_dict = {\n",
    "    'ssd': torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd'),\n",
    "    'lenet': torch.hub.load('pytorch/vision:v0.10.0', 'googlenet'),\n",
    "    'yolov5s': torch.hub.load('ultralytics/yolov5', 'yolov5s'),\n",
    "    'yolov5m': torch.hub.load('ultralytics/yolov5', 'yolov5m'),\n",
    "    'yolov5l': torch.hub.load('ultralytics/yolov5', 'yolov5l'),\n",
    "    'yolov5x': torch.hub.load('ultralytics/yolov5', 'yolov5x'),\n",
    "    'alexnet': models.alexnet(),\n",
    "    'squeezenet_1_0': models.squeezenet1_1(),\n",
    "    'squeezenet_1_1': models.squeezenet1_0(),\n",
    "    'googlenet': models.googlenet(),\n",
    "    'shufflenet_v2_x0_5': models.shufflenet_v2_x0_5(),\n",
    "    'shufflenet_v2_x1_0': models.shufflenet_v2_x1_0(),\n",
    "}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/sultan/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "Using cache found in /home/sultan/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/sultan/CNN_Network_Analysis/.venv/lib/python3.9/site-packages/torchvision/models/googlenet.py:77: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7266973 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 308 layers, 21356877 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 392 layers, 47025981 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in /home/sultan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2021-8-29 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2070 SUPER, 7979.1875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 87730285 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "/home/sultan/CNN_Network_Analysis/.venv/lib/python3.9/site-packages/torchvision/models/googlenet.py:77: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# prepare sample inputs\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "uris = [\n",
    "    'http://images.cocodataset.org/val2017/000000397133.jpg'\n",
    "]\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n",
    "\n",
    "inputs = [utils.prepare_input(uri) for uri in uris]\n",
    "ssd_input_batch = utils.prepare_tensor(inputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/sultan/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "stats_dict = ModelStatAnalyser.get_models_stats_dict(model_dict, input_batch, ssd_input_batch) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model_batch_size = 4\n",
    "model_list = timm.list_models(exclude_filters=['levit_*'], pretrained=False)\n",
    "in_shapes = []\n",
    "for model_batch_idx in tqdm(range(0, len(model_list), model_batch_size)):\n",
    "    for model_name in model_list[model_batch_idx: model_batch_idx+model_batch_size]:\n",
    "        model = timm.create_model(model_name, pretrained=False)\n",
    "        config = resolve_data_config({}, model=model)\n",
    "        transform = create_transform(**config)\n",
    "\n",
    "        url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        img = Image.open(filename).convert('RGB')\n",
    "        tensor = transform(img).unsqueeze(0) # transform and add batch dimension\n",
    "        in_tensor_shape = tensor.size()\n",
    "        if in_tensor_shape[2] == 224 and in_tensor_shape[3] == 224:       \n",
    "            in_shapes.append(in_tensor_shape)\n",
    "            model_dict = {}\n",
    "            model_dict[model_name] = model\n",
    "            try:\n",
    "                res_stats_dict = ModelStatAnalyser.get_models_stats_dict(model_dict, tensor, ssd_input_batch)\n",
    "            except Exception as e:\n",
    "                print(f\"Model name {model_name}\")\n",
    "                raise e\n",
    "            stats_dict = stats_dict | res_stats_dict"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/152 [00:00<?, ?it/s]/home/sultan/CNN_Network_Analysis/.venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [07:36<00:00,  3.00s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "total_networks = len(stats_dict.keys())\n",
    "networks_with_kernel = {f'{k}': 0 for k in range(1, 12)}\n",
    "for network, stats in stats_dict.items():\n",
    "    for kernel_size in networks_with_kernel.keys():\n",
    "        if kernel_size in stats['kernel']._dict:\n",
    "            networks_with_kernel[kernel_size] += 1\n",
    "print(networks_with_kernel)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'1': 336, '2': 13, '3': 356, '4': 19, '5': 66, '6': 0, '7': 156, '8': 11, '9': 8, '10': 0, '11': 3}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from numpy import linspace\n",
    "from math import floor\n",
    "longest_network_length = max([len(stats['raw_stats'].keys()) for model, stats in stats_dict.items()])\n",
    "normalized_model_intermediate_layer_size = {}\n",
    "for model, stats in stats_dict.items():\n",
    "    normalized_model_intermediate_layer_size[model] = [0]*longest_network_length\n",
    "    model_length = len(stats['intermediate_layer_sizes'])\n",
    "    idx_list, step = linspace(0, longest_network_length, model_length, retstep=True)\n",
    "    idx_list, step = [floor(idx) for idx in idx_list], floor(step)\n",
    "    input_layers = stats['intermediate_layer_sizes'][0:-1]\n",
    "    output_layers = stats['intermediate_layer_sizes'][1:]\n",
    "    layer_sizes = zip(input_layers, output_layers)\n",
    "    for idx in idx_list[:-1]:\n",
    "        try:\n",
    "            in_layer, out_layer = next(layer_sizes)\n",
    "        except:\n",
    "            print(idx)\n",
    "            raise Exception('WOOPS!')\n",
    "        try:\n",
    "            in_layer, out_layer = in_layer[1][-1], out_layer[1][-1]\n",
    "        except:\n",
    "            print(in_layer)\n",
    "            print(out_layer)\n",
    "            raise Exception('WOOPS!')\n",
    "        interpolated_layers = [floor(layer_size) for layer_size in linspace(in_layer, out_layer, step)]\n",
    "        for interpolation_idx in range(idx, idx+step):\n",
    "            try:\n",
    "                normalized_model_intermediate_layer_size[model][interpolation_idx] = interpolated_layers[interpolation_idx-idx] \n",
    "            except:\n",
    "                print(interpolation_idx)\n",
    "                raise Exception('WOOPS')\n",
    "    # interpolate ith linspace + linspace step size when length of networks < max length\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "normalized_model_intermediate_layer_size['alexnet']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[224,\n",
       " 220,\n",
       " 217,\n",
       " 214,\n",
       " 211,\n",
       " 208,\n",
       " 205,\n",
       " 202,\n",
       " 199,\n",
       " 196,\n",
       " 193,\n",
       " 190,\n",
       " 187,\n",
       " 183,\n",
       " 180,\n",
       " 177,\n",
       " 174,\n",
       " 171,\n",
       " 168,\n",
       " 165,\n",
       " 162,\n",
       " 159,\n",
       " 156,\n",
       " 153,\n",
       " 150,\n",
       " 147,\n",
       " 143,\n",
       " 140,\n",
       " 137,\n",
       " 134,\n",
       " 131,\n",
       " 128,\n",
       " 125,\n",
       " 122,\n",
       " 119,\n",
       " 116,\n",
       " 113,\n",
       " 110,\n",
       " 107,\n",
       " 103,\n",
       " 100,\n",
       " 97,\n",
       " 94,\n",
       " 91,\n",
       " 88,\n",
       " 85,\n",
       " 82,\n",
       " 79,\n",
       " 76,\n",
       " 73,\n",
       " 70,\n",
       " 67,\n",
       " 63,\n",
       " 60,\n",
       " 57,\n",
       " 54,\n",
       " 51,\n",
       " 48,\n",
       " 45,\n",
       " 42,\n",
       " 39,\n",
       " 36,\n",
       " 33,\n",
       " 30,\n",
       " 27,\n",
       " 27,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "ModelStatsAggregator.get_aggregate_kernel_stats_as_percentages(stats_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'7': 0.008996066358816487,\n",
       " '1': 0.6101932615016248,\n",
       " '3': 0.35016247648366683,\n",
       " '11': 0.0001026167265264238,\n",
       " '5': 0.020489139729775955,\n",
       " '16': 0.001573456473405165,\n",
       " '4': 0.0012998118693347016,\n",
       " '2': 0.003728407730460065,\n",
       " '32': 0.0003078501795792714,\n",
       " '9': 0.0024628014366341712,\n",
       " '14': 0.0001026167265264238,\n",
       " '8': 0.000581494783649735}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "expected_kernel_sizes = list(range(1,12))\n",
    "count_of_models_with_kernel_size = {f'{k}': 0 for k in expected_kernel_sizes}\n",
    "for model, stats in stats_dict.items():\n",
    "    for size in expected_kernel_sizes:\n",
    "        count_of_models_with_kernel_size[f'{size}'] += (1 if stats['kernel'][f'{size}'] > 0 else 0)\n",
    "print(count_of_models_with_kernel_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'1': 380, '2': 22, '3': 359, '4': 22, '5': 66, '6': 46, '7': 156, '8': 11, '9': 8, '10': 0, '11': 0}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "stats_dict['ssd']['intermediate_layer_sizes']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(270000, [1, 3, 300, 300]),\n",
       " (360000, [1, 64, 75, 75]),\n",
       " (360000, [1, 64, 75, 75]),\n",
       " (360000, [1, 64, 75, 75]),\n",
       " (360000, [1, 64, 75, 75]),\n",
       " (1440000, [1, 256, 75, 75]),\n",
       " (360000, [1, 64, 75, 75]),\n",
       " (360000, [1, 64, 75, 75]),\n",
       " (1440000, [1, 256, 75, 75]),\n",
       " (360000, [1, 64, 75, 75]),\n",
       " (360000, [1, 64, 75, 75]),\n",
       " (1440000, [1, 256, 75, 75]),\n",
       " (720000, [1, 128, 75, 75]),\n",
       " (184832, [1, 128, 38, 38]),\n",
       " (1440000, [1, 256, 75, 75]),\n",
       " (739328, [1, 512, 38, 38]),\n",
       " (184832, [1, 128, 38, 38]),\n",
       " (184832, [1, 128, 38, 38]),\n",
       " (739328, [1, 512, 38, 38]),\n",
       " (184832, [1, 128, 38, 38]),\n",
       " (184832, [1, 128, 38, 38]),\n",
       " (739328, [1, 512, 38, 38]),\n",
       " (184832, [1, 128, 38, 38]),\n",
       " (184832, [1, 128, 38, 38]),\n",
       " (739328, [1, 512, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (739328, [1, 512, 38, 38]),\n",
       " (1478656, [1, 1024, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (1478656, [1, 1024, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (1478656, [1, 1024, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (1478656, [1, 1024, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (1478656, [1, 1024, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (1478656, [1, 1024, 38, 38]),\n",
       " (369664, [1, 256, 38, 38]),\n",
       " (184832, [1, 512, 19, 19]),\n",
       " (92416, [1, 256, 19, 19]),\n",
       " (51200, [1, 512, 10, 10]),\n",
       " (12800, [1, 128, 10, 10]),\n",
       " (6400, [1, 256, 5, 5]),\n",
       " (3200, [1, 128, 5, 5]),\n",
       " (2304, [1, 256, 3, 3]),\n",
       " (1152, [1, 128, 3, 3]),\n",
       " (1478656, [1, 1024, 38, 38]),\n",
       " (1478656, [1, 1024, 38, 38]),\n",
       " (184832, [1, 512, 19, 19]),\n",
       " (184832, [1, 512, 19, 19]),\n",
       " (51200, [1, 512, 10, 10]),\n",
       " (51200, [1, 512, 10, 10]),\n",
       " (6400, [1, 256, 5, 5]),\n",
       " (6400, [1, 256, 5, 5]),\n",
       " (2304, [1, 256, 3, 3]),\n",
       " (2304, [1, 256, 3, 3]),\n",
       " (256, [1, 256, 1, 1]),\n",
       " (256, [1, 256, 1, 1]),\n",
       " (324, [324, 1, 1])]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('.venv': poetry)"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "metadata": {
   "interpreter": {
    "hash": "7ba5a8c11295bd31a650da7fd95bdbf05f90d3e784de664cc7b5246ae59e510d"
   }
  },
  "orig_nbformat": 2,
  "interpreter": {
   "hash": "7ba5a8c11295bd31a650da7fd95bdbf05f90d3e784de664cc7b5246ae59e510d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}